{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_and_labels(data_dir):\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for d in os.listdir(data_dir):\n",
    "        category_filenames = glob(f'{data_dir}/{d}/*')\n",
    "        filenames.extend(category_filenames)\n",
    "        labels.extend([d for x in range(len(category_filenames))])\n",
    "\n",
    "    mapping = {l: idx for idx, l in enumerate(np.unique(labels))}\n",
    "\n",
    "    category = [mapping[l] for l in labels]\n",
    "\n",
    "    category = tf.keras.utils.to_categorical(category, len(np.unique(labels)))\n",
    "    \n",
    "    return filenames, category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(filenames, labels, shuffle_buffer_size=10000, n_repeat_times=100, batch_size=128, shuffle=True):\n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        image = tf.cast(image_decoded, tf.float32)\n",
    "        # resized_images = tf.image.resize_images(images, (img_size, img_size))\n",
    "        resized_image = tf.image.resize_images(image, (img_size, img_size))\n",
    "        # resized_image = tf.reshape(\n",
    "        #     resized_image,\n",
    "        #     (img_size * img_size * 3, ))\n",
    "        return resized_image/255, label\n",
    "    \n",
    "    len_filenames = len(filenames)\n",
    "    \n",
    "    tuples = [(f, l) for f, l in zip(filenames, labels)]\n",
    "    random.seed(14)\n",
    "    random.shuffle(tuples)\n",
    "    \n",
    "    filenames = [t[0] for t in tuples]\n",
    "    labels = np.array([t[1] for t in tuples])\n",
    "    \n",
    "    # step 1\n",
    "    filenames = tf.constant(filenames)\n",
    "    labels = tf.constant(labels)\n",
    "\n",
    "    # step 2: create a dataset returning slices of `filenames`\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "\n",
    "    # step 3: parse every image in the dataset using `map`\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size,\n",
    "                                  reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat(n_repeat_times).batch(batch_size if shuffle else 1024)\n",
    "\n",
    "    # step 4: create iterator and final input tensor\n",
    "    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "    images, labels = iterator.get_next()\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convolutional Neural Network.\n",
    "Build and train a convolutional neural network with TensorFlow.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "num_classes = 6 \n",
    "img_size = 128\n",
    "channels = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "display_step = 50\n",
    "\n",
    "num_input = (img_size * img_size * channels) \n",
    "dropout = 0.5 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, img_size, img_size, channels], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes], name='y')\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # x = tf.reshape(x, shape=[-1, 224, 224, 3]) \n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['block_1_wc1'], biases['block_1_bc1'])\n",
    "    conv1 = tf.compat.v1.layers.batch_normalization(conv1, \n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    conv1 = conv2d(conv1, weights['block_1_wc2'], biases['block_1_bc2'])\n",
    "    conv1 = tf.compat.v1.layers.batch_normalization(conv1,\n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    # Apply Dropout\n",
    "    conv1 = tf.nn.dropout(conv1, 1 if dropout == 1 else 0.75)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['block_2_wc1'], biases['block_2_bc1'])\n",
    "    conv2 = tf.compat.v1.layers.batch_normalization(conv2, \n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    conv2 = conv2d(conv2, weights['block_2_wc2'], biases['block_2_bc2'])\n",
    "    conv2 = tf.compat.v1.layers.batch_normalization(conv2,\n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Apply Dropout\n",
    "    conv2 = tf.nn.dropout(conv2, 1 if dropout == 1 else 0.75)\n",
    "        \n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    flatten = tf.contrib.layers.flatten(conv2)\n",
    "    fc1 = tf.contrib.layers.fully_connected(flatten, 512, )\n",
    "\n",
    "    # fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    # fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    # fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'block_1_wc1': tf.Variable(tf.random_normal([3, 3, 3, 32])),\n",
    "    'block_1_wc2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    \n",
    "    'block_2_wc1': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
    "    'block_2_wc2': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    \n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 512])),\n",
    "    # 1024 inputs, 6 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([512, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'block_1_bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'block_1_bc2': tf.Variable(tf.random_normal([32])),\n",
    "    'block_2_bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'block_2_bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([512])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data.PATH()\n",
    "dataset = 'vision_based_dataset'\n",
    "base_dir = f'{path.PROCESSED_DATA_PATH}/{dataset}'\n",
    "train_dir = f'{base_dir}/train/'\n",
    "validation_dir = f'{base_dir}/validation/'\n",
    "test_dir = f'{base_dir}/test/'\n",
    "\n",
    "filenames_train, labels_train = get_filenames_and_labels(train_dir)\n",
    "filenames_valid, labels_valid = get_filenames_and_labels(validation_dir)\n",
    "\n",
    "train_images, train_labels = create_tf_dataset(filenames_train, labels_train, batch_size=batch_size)\n",
    "\n",
    "validation_images, validation_labels = create_tf_dataset(filenames_valid, labels_valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = int(num_epochs * len(filenames_train) / batch_size) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1201 14:46:34.478761 140145065584448 deprecation.py:323] From <ipython-input-9-ef1fea106582>:21: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W1201 14:46:34.591379 140145065584448 deprecation.py:506] From <ipython-input-9-ef1fea106582>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1201 14:46:35.733103 140145065584448 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1201 14:46:35.734665 140145065584448 deprecation.py:323] From /home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "W1201 14:46:36.193791 140145065584448 deprecation.py:323] From <ipython-input-12-01f016bd9dd3>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/178688 (64 imgs) >> minibatch loss: 1100.3299560546875; acc 0.4699999988079071; val_loss: 1810.02001953125; val_acc 0.27000001072883606 (Total mins: 0.59)\n",
      "Step 50/178688 (3200 imgs) >> minibatch loss: 1.9600000381469727; acc 0.2199999988079071; val_loss: 2.6600000858306885; val_acc 0.2199999988079071 (Total mins: 0.84)\n",
      "Step 100/178688 (6400 imgs) >> minibatch loss: 2.0; acc 0.20000000298023224; val_loss: 2.109999895095825; val_acc 0.1899999976158142 (Total mins: 1.1)\n",
      "Step 150/178688 (9600 imgs) >> minibatch loss: 2.049999952316284; acc 0.27000001072883606; val_loss: 2.049999952316284; val_acc 0.18000000715255737 (Total mins: 1.34)\n",
      "Step 200/178688 (12800 imgs) >> minibatch loss: 1.809999942779541; acc 0.25; val_loss: 2.0; val_acc 0.1899999976158142 (Total mins: 1.6)\n",
      "Step 250/178688 (16000 imgs) >> minibatch loss: 2.0899999141693115; acc 0.20000000298023224; val_loss: 1.9600000381469727; val_acc 0.1899999976158142 (Total mins: 1.85)\n",
      "Step 300/178688 (19200 imgs) >> minibatch loss: 1.7999999523162842; acc 0.30000001192092896; val_loss: 1.940000057220459; val_acc 0.20999999344348907 (Total mins: 2.1)\n",
      "Step 350/178688 (22400 imgs) >> minibatch loss: 1.9600000381469727; acc 0.2800000011920929; val_loss: 1.9600000381469727; val_acc 0.2199999988079071 (Total mins: 2.34)\n",
      "Step 400/178688 (25600 imgs) >> minibatch loss: 2.0; acc 0.17000000178813934; val_loss: 1.940000057220459; val_acc 0.20000000298023224 (Total mins: 2.59)\n",
      "Step 450/178688 (28800 imgs) >> minibatch loss: 1.9500000476837158; acc 0.1899999976158142; val_loss: 1.8799999952316284; val_acc 0.18000000715255737 (Total mins: 2.83)\n",
      "Step 500/178688 (32000 imgs) >> minibatch loss: 1.809999942779541; acc 0.1899999976158142; val_loss: 1.9199999570846558; val_acc 0.18000000715255737 (Total mins: 3.08)\n",
      "Step 550/178688 (35200 imgs) >> minibatch loss: 2.119999885559082; acc 0.09000000357627869; val_loss: 1.8600000143051147; val_acc 0.1899999976158142 (Total mins: 3.33)\n",
      "Step 600/178688 (38400 imgs) >> minibatch loss: 1.9299999475479126; acc 0.17000000178813934; val_loss: 1.8700000047683716; val_acc 0.20000000298023224 (Total mins: 3.57)\n",
      "Step 650/178688 (41600 imgs) >> minibatch loss: 1.8899999856948853; acc 0.20000000298023224; val_loss: 1.940000057220459; val_acc 0.20999999344348907 (Total mins: 3.82)\n",
      "Step 700/178688 (44800 imgs) >> minibatch loss: 1.8200000524520874; acc 0.1899999976158142; val_loss: 1.8200000524520874; val_acc 0.20999999344348907 (Total mins: 4.06)\n",
      "Step 750/178688 (48000 imgs) >> minibatch loss: 1.7899999618530273; acc 0.1599999964237213; val_loss: 1.8200000524520874; val_acc 0.1899999976158142 (Total mins: 4.3)\n",
      "Step 800/178688 (51200 imgs) >> minibatch loss: 1.940000057220459; acc 0.20000000298023224; val_loss: 1.840000033378601; val_acc 0.1899999976158142 (Total mins: 4.55)\n",
      "Step 850/178688 (54400 imgs) >> minibatch loss: 1.8799999952316284; acc 0.17000000178813934; val_loss: 1.809999942779541; val_acc 0.1899999976158142 (Total mins: 4.8)\n",
      "Step 900/178688 (57600 imgs) >> minibatch loss: 1.8700000047683716; acc 0.09000000357627869; val_loss: 1.9199999570846558; val_acc 0.1899999976158142 (Total mins: 5.05)\n",
      "Step 950/178688 (60800 imgs) >> minibatch loss: 1.7599999904632568; acc 0.25; val_loss: 1.8899999856948853; val_acc 0.2199999988079071 (Total mins: 5.3)\n",
      "Step 1000/178688 (64000 imgs) >> minibatch loss: 1.7799999713897705; acc 0.25; val_loss: 1.7999999523162842; val_acc 0.20999999344348907 (Total mins: 5.55)\n",
      "Step 1050/178688 (67200 imgs) >> minibatch loss: 1.7799999713897705; acc 0.25; val_loss: 1.7899999618530273; val_acc 0.1899999976158142 (Total mins: 5.79)\n",
      "Step 1100/178688 (70400 imgs) >> minibatch loss: 1.7999999523162842; acc 0.17000000178813934; val_loss: 1.7899999618530273; val_acc 0.18000000715255737 (Total mins: 6.04)\n",
      "Step 1150/178688 (73600 imgs) >> minibatch loss: 1.7100000381469727; acc 0.2199999988079071; val_loss: 1.7799999713897705; val_acc 0.1899999976158142 (Total mins: 6.29)\n",
      "Step 1200/178688 (76800 imgs) >> minibatch loss: 1.8600000143051147; acc 0.10999999940395355; val_loss: 1.8600000143051147; val_acc 0.1899999976158142 (Total mins: 6.54)\n",
      "Step 1250/178688 (80000 imgs) >> minibatch loss: 1.7599999904632568; acc 0.25; val_loss: 1.850000023841858; val_acc 0.2199999988079071 (Total mins: 6.78)\n",
      "Step 1300/178688 (83200 imgs) >> minibatch loss: 1.7300000190734863; acc 0.3100000023841858; val_loss: 1.8300000429153442; val_acc 0.20999999344348907 (Total mins: 7.03)\n",
      "Step 1350/178688 (86400 imgs) >> minibatch loss: 1.7799999713897705; acc 0.17000000178813934; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 7.27)\n",
      "Step 1400/178688 (89600 imgs) >> minibatch loss: 1.7400000095367432; acc 0.33000001311302185; val_loss: 1.7599999904632568; val_acc 0.18000000715255737 (Total mins: 7.51)\n",
      "Step 1450/178688 (92800 imgs) >> minibatch loss: 1.7300000190734863; acc 0.25; val_loss: 1.7799999713897705; val_acc 0.18000000715255737 (Total mins: 7.76)\n",
      "Step 1500/178688 (96000 imgs) >> minibatch loss: 1.75; acc 0.17000000178813934; val_loss: 1.7699999809265137; val_acc 0.1899999976158142 (Total mins: 8.0)\n",
      "Step 1550/178688 (99200 imgs) >> minibatch loss: 1.7300000190734863; acc 0.1899999976158142; val_loss: 1.7599999904632568; val_acc 0.2199999988079071 (Total mins: 8.24)\n",
      "Step 1600/178688 (102400 imgs) >> minibatch loss: 1.75; acc 0.2199999988079071; val_loss: 1.7599999904632568; val_acc 0.20999999344348907 (Total mins: 8.49)\n",
      "Step 1650/178688 (105600 imgs) >> minibatch loss: 1.7400000095367432; acc 0.1899999976158142; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 8.73)\n",
      "Step 1700/178688 (108800 imgs) >> minibatch loss: 1.7599999904632568; acc 0.25; val_loss: 1.7599999904632568; val_acc 0.17000000178813934 (Total mins: 8.98)\n",
      "Step 1750/178688 (112000 imgs) >> minibatch loss: 1.7799999713897705; acc 0.10999999940395355; val_loss: 1.7899999618530273; val_acc 0.18000000715255737 (Total mins: 9.22)\n",
      "Step 1800/178688 (115200 imgs) >> minibatch loss: 1.7599999904632568; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 9.47)\n",
      "Step 1850/178688 (118400 imgs) >> minibatch loss: 1.7899999618530273; acc 0.11999999731779099; val_loss: 1.7599999904632568; val_acc 0.20999999344348907 (Total mins: 9.72)\n",
      "Step 1900/178688 (121600 imgs) >> minibatch loss: 1.7899999618530273; acc 0.23000000417232513; val_loss: 1.75; val_acc 0.2199999988079071 (Total mins: 9.99)\n",
      "Step 1950/178688 (124800 imgs) >> minibatch loss: 1.7200000286102295; acc 0.27000001072883606; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 10.24)\n",
      "Step 2000/178688 (128000 imgs) >> minibatch loss: 1.75; acc 0.14000000059604645; val_loss: 1.75; val_acc 0.17000000178813934 (Total mins: 10.48)\n",
      "Step 2050/178688 (131200 imgs) >> minibatch loss: 1.840000033378601; acc 0.1599999964237213; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 10.72)\n",
      "Step 2100/178688 (134400 imgs) >> minibatch loss: 1.7300000190734863; acc 0.17000000178813934; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 10.96)\n",
      "Step 2150/178688 (137600 imgs) >> minibatch loss: 1.7400000095367432; acc 0.1899999976158142; val_loss: 1.7599999904632568; val_acc 0.20999999344348907 (Total mins: 11.21)\n",
      "Step 2200/178688 (140800 imgs) >> minibatch loss: 1.7699999809265137; acc 0.10999999940395355; val_loss: 1.7400000095367432; val_acc 0.2199999988079071 (Total mins: 11.47)\n",
      "Step 2250/178688 (144000 imgs) >> minibatch loss: 1.7599999904632568; acc 0.17000000178813934; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 11.74)\n",
      "Step 2300/178688 (147200 imgs) >> minibatch loss: 1.7300000190734863; acc 0.20000000298023224; val_loss: 1.7400000095367432; val_acc 0.18000000715255737 (Total mins: 12.0)\n",
      "Step 2350/178688 (150400 imgs) >> minibatch loss: 1.7599999904632568; acc 0.20000000298023224; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 12.28)\n",
      "Step 2400/178688 (153600 imgs) >> minibatch loss: 1.7400000095367432; acc 0.2800000011920929; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 12.55)\n",
      "Step 2450/178688 (156800 imgs) >> minibatch loss: 1.7799999713897705; acc 0.14000000059604645; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 12.88)\n",
      "Step 2500/178688 (160000 imgs) >> minibatch loss: 1.7599999904632568; acc 0.11999999731779099; val_loss: 1.75; val_acc 0.2199999988079071 (Total mins: 13.15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2550/178688 (163200 imgs) >> minibatch loss: 1.7899999618530273; acc 0.27000001072883606; val_loss: 1.7400000095367432; val_acc 0.20000000298023224 (Total mins: 13.42)\n",
      "Step 2600/178688 (166400 imgs) >> minibatch loss: 1.7799999713897705; acc 0.2800000011920929; val_loss: 1.7400000095367432; val_acc 0.20000000298023224 (Total mins: 13.69)\n",
      "Step 2650/178688 (169600 imgs) >> minibatch loss: 1.75; acc 0.23000000417232513; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 13.94)\n",
      "Step 2700/178688 (172800 imgs) >> minibatch loss: 1.809999942779541; acc 0.25; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 14.21)\n",
      "Step 2750/178688 (176000 imgs) >> minibatch loss: 1.7200000286102295; acc 0.2199999988079071; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 14.46)\n",
      "Step 2800/178688 (179200 imgs) >> minibatch loss: 1.7100000381469727; acc 0.25; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 14.71)\n",
      "Step 2850/178688 (182400 imgs) >> minibatch loss: 1.7599999904632568; acc 0.10999999940395355; val_loss: 1.7400000095367432; val_acc 0.20999999344348907 (Total mins: 14.96)\n",
      "Step 2900/178688 (185600 imgs) >> minibatch loss: 1.8300000429153442; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 15.23)\n",
      "Step 2950/178688 (188800 imgs) >> minibatch loss: 1.8300000429153442; acc 0.23000000417232513; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 15.5)\n",
      "Step 3000/178688 (192000 imgs) >> minibatch loss: 1.7400000095367432; acc 0.23000000417232513; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 15.79)\n",
      "Step 3050/178688 (195200 imgs) >> minibatch loss: 1.809999942779541; acc 0.11999999731779099; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 16.04)\n",
      "Step 3100/178688 (198400 imgs) >> minibatch loss: 1.7400000095367432; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 16.29)\n",
      "Step 3150/178688 (201600 imgs) >> minibatch loss: 1.7699999809265137; acc 0.1899999976158142; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 16.54)\n",
      "Step 3200/178688 (204800 imgs) >> minibatch loss: 1.7000000476837158; acc 0.23000000417232513; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 16.79)\n",
      "Step 3250/178688 (208000 imgs) >> minibatch loss: 1.75; acc 0.17000000178813934; val_loss: 1.7400000095367432; val_acc 0.20999999344348907 (Total mins: 17.03)\n",
      "Step 3300/178688 (211200 imgs) >> minibatch loss: 1.75; acc 0.1599999964237213; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 17.28)\n",
      "Step 3350/178688 (214400 imgs) >> minibatch loss: 1.7599999904632568; acc 0.2800000011920929; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 17.55)\n",
      "Step 3400/178688 (217600 imgs) >> minibatch loss: 1.7400000095367432; acc 0.11999999731779099; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 17.84)\n",
      "Step 3450/178688 (220800 imgs) >> minibatch loss: 1.7400000095367432; acc 0.17000000178813934; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 18.11)\n",
      "Step 3500/178688 (224000 imgs) >> minibatch loss: 1.7699999809265137; acc 0.30000001192092896; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 18.38)\n",
      "Step 3550/178688 (227200 imgs) >> minibatch loss: 1.7200000286102295; acc 0.17000000178813934; val_loss: 1.75; val_acc 0.2199999988079071 (Total mins: 18.67)\n",
      "Step 3600/178688 (230400 imgs) >> minibatch loss: 1.7799999713897705; acc 0.10999999940395355; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 18.96)\n",
      "Step 3650/178688 (233600 imgs) >> minibatch loss: 1.7699999809265137; acc 0.17000000178813934; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 19.23)\n",
      "Step 3700/178688 (236800 imgs) >> minibatch loss: 1.7899999618530273; acc 0.1899999976158142; val_loss: 1.7599999904632568; val_acc 0.17000000178813934 (Total mins: 19.51)\n",
      "Step 3750/178688 (240000 imgs) >> minibatch loss: 1.7300000190734863; acc 0.11999999731779099; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 19.78)\n",
      "Step 3800/178688 (243200 imgs) >> minibatch loss: 1.75; acc 0.1899999976158142; val_loss: 1.7400000095367432; val_acc 0.20999999344348907 (Total mins: 20.06)\n",
      "Step 3850/178688 (246400 imgs) >> minibatch loss: 1.7799999713897705; acc 0.1599999964237213; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 20.32)\n",
      "Step 3900/178688 (249600 imgs) >> minibatch loss: 1.7699999809265137; acc 0.17000000178813934; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 20.62)\n",
      "Step 3950/178688 (252800 imgs) >> minibatch loss: 1.7899999618530273; acc 0.20000000298023224; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 20.91)\n",
      "Step 4000/178688 (256000 imgs) >> minibatch loss: 1.75; acc 0.2199999988079071; val_loss: 1.7599999904632568; val_acc 0.20999999344348907 (Total mins: 21.18)\n",
      "Step 4050/178688 (259200 imgs) >> minibatch loss: 1.7100000381469727; acc 0.3799999952316284; val_loss: 1.75; val_acc 0.18000000715255737 (Total mins: 21.44)\n",
      "Step 4100/178688 (262400 imgs) >> minibatch loss: 1.7300000190734863; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 21.69)\n",
      "Step 4150/178688 (265600 imgs) >> minibatch loss: 1.75; acc 0.23000000417232513; val_loss: 1.7400000095367432; val_acc 0.20999999344348907 (Total mins: 21.93)\n",
      "Step 4200/178688 (268800 imgs) >> minibatch loss: 1.7699999809265137; acc 0.27000001072883606; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 22.17)\n",
      "Step 4250/178688 (272000 imgs) >> minibatch loss: 1.7999999523162842; acc 0.1899999976158142; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 22.42)\n",
      "Step 4300/178688 (275200 imgs) >> minibatch loss: 1.7100000381469727; acc 0.23000000417232513; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 22.66)\n",
      "Step 4350/178688 (278400 imgs) >> minibatch loss: 1.7300000190734863; acc 0.23000000417232513; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 22.9)\n",
      "Step 4400/178688 (281600 imgs) >> minibatch loss: 1.7400000095367432; acc 0.23000000417232513; val_loss: 1.75; val_acc 0.18000000715255737 (Total mins: 23.14)\n",
      "Step 4450/178688 (284800 imgs) >> minibatch loss: 1.7699999809265137; acc 0.23000000417232513; val_loss: 1.7400000095367432; val_acc 0.20000000298023224 (Total mins: 23.38)\n",
      "Step 4500/178688 (288000 imgs) >> minibatch loss: 1.7899999618530273; acc 0.20000000298023224; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 23.63)\n",
      "Step 4550/178688 (291200 imgs) >> minibatch loss: 1.7100000381469727; acc 0.1899999976158142; val_loss: 1.8200000524520874; val_acc 0.23000000417232513 (Total mins: 23.88)\n",
      "Step 4600/178688 (294400 imgs) >> minibatch loss: 1.7100000381469727; acc 0.17000000178813934; val_loss: 1.7699999809265137; val_acc 0.20000000298023224 (Total mins: 24.12)\n",
      "Step 4650/178688 (297600 imgs) >> minibatch loss: 1.7200000286102295; acc 0.30000001192092896; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 24.36)\n",
      "Step 4700/178688 (300800 imgs) >> minibatch loss: 1.7599999904632568; acc 0.23000000417232513; val_loss: 1.7400000095367432; val_acc 0.20000000298023224 (Total mins: 24.6)\n",
      "Step 4750/178688 (304000 imgs) >> minibatch loss: 1.7300000190734863; acc 0.1899999976158142; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 24.85)\n",
      "Step 4800/178688 (307200 imgs) >> minibatch loss: 1.7200000286102295; acc 0.33000001311302185; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 25.09)\n",
      "Step 4850/178688 (310400 imgs) >> minibatch loss: 1.7300000190734863; acc 0.23000000417232513; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 25.34)\n",
      "Step 4900/178688 (313600 imgs) >> minibatch loss: 1.75; acc 0.2199999988079071; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 25.58)\n",
      "Step 4950/178688 (316800 imgs) >> minibatch loss: 1.7599999904632568; acc 0.25; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 25.82)\n",
      "Step 5000/178688 (320000 imgs) >> minibatch loss: 1.7799999713897705; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 26.06)\n",
      "Step 5050/178688 (323200 imgs) >> minibatch loss: 1.75; acc 0.23000000417232513; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 26.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5100/178688 (326400 imgs) >> minibatch loss: 1.7599999904632568; acc 0.2199999988079071; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 26.54)\n",
      "Step 5150/178688 (329600 imgs) >> minibatch loss: 1.75; acc 0.2800000011920929; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 26.83)\n",
      "Step 5200/178688 (332800 imgs) >> minibatch loss: 1.7200000286102295; acc 0.20000000298023224; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 27.11)\n",
      "Step 5250/178688 (336000 imgs) >> minibatch loss: 1.7999999523162842; acc 0.10999999940395355; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 27.43)\n",
      "Step 5300/178688 (339200 imgs) >> minibatch loss: 1.7300000190734863; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 27.7)\n",
      "Step 5350/178688 (342400 imgs) >> minibatch loss: 1.7599999904632568; acc 0.11999999731779099; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 27.96)\n",
      "Step 5400/178688 (345600 imgs) >> minibatch loss: 1.7999999523162842; acc 0.09000000357627869; val_loss: 1.75; val_acc 0.2199999988079071 (Total mins: 28.21)\n",
      "Step 5450/178688 (348800 imgs) >> minibatch loss: 1.7100000381469727; acc 0.23000000417232513; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 28.47)\n",
      "Step 5500/178688 (352000 imgs) >> minibatch loss: 1.75; acc 0.23000000417232513; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 28.73)\n",
      "Step 5550/178688 (355200 imgs) >> minibatch loss: 1.7000000476837158; acc 0.17000000178813934; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 28.97)\n",
      "Step 5600/178688 (358400 imgs) >> minibatch loss: 1.7100000381469727; acc 0.1599999964237213; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 29.22)\n",
      "Step 5650/178688 (361600 imgs) >> minibatch loss: 1.7999999523162842; acc 0.11999999731779099; val_loss: 1.7400000095367432; val_acc 0.20000000298023224 (Total mins: 29.49)\n",
      "Step 5700/178688 (364800 imgs) >> minibatch loss: 1.7599999904632568; acc 0.17000000178813934; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 29.75)\n",
      "Step 5750/178688 (368000 imgs) >> minibatch loss: 1.75; acc 0.1599999964237213; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 30.01)\n",
      "Step 5800/178688 (371200 imgs) >> minibatch loss: 1.7300000190734863; acc 0.11999999731779099; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 30.26)\n",
      "Step 5850/178688 (374400 imgs) >> minibatch loss: 1.7300000190734863; acc 0.25; val_loss: 1.7599999904632568; val_acc 0.1599999964237213 (Total mins: 30.5)\n",
      "Step 5900/178688 (377600 imgs) >> minibatch loss: 1.7300000190734863; acc 0.1899999976158142; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 30.76)\n",
      "Step 5950/178688 (380800 imgs) >> minibatch loss: 1.7300000190734863; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 31.0)\n",
      "Step 6000/178688 (384000 imgs) >> minibatch loss: 1.7799999713897705; acc 0.1899999976158142; val_loss: 1.75; val_acc 0.20999999344348907 (Total mins: 31.25)\n",
      "Step 6050/178688 (387200 imgs) >> minibatch loss: 1.7599999904632568; acc 0.2199999988079071; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 31.5)\n",
      "Step 6100/178688 (390400 imgs) >> minibatch loss: 1.7899999618530273; acc 0.2199999988079071; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 31.77)\n",
      "Step 6150/178688 (393600 imgs) >> minibatch loss: 1.7799999713897705; acc 0.1599999964237213; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 32.02)\n",
      "Step 6200/178688 (396800 imgs) >> minibatch loss: 1.7699999809265137; acc 0.14000000059604645; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 32.27)\n",
      "Step 6250/178688 (400000 imgs) >> minibatch loss: 1.75; acc 0.2199999988079071; val_loss: 1.75; val_acc 0.18000000715255737 (Total mins: 32.52)\n",
      "Step 6300/178688 (403200 imgs) >> minibatch loss: 1.7599999904632568; acc 0.1899999976158142; val_loss: 1.7400000095367432; val_acc 0.18000000715255737 (Total mins: 32.77)\n",
      "Step 6350/178688 (406400 imgs) >> minibatch loss: 1.7599999904632568; acc 0.20000000298023224; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 33.03)\n",
      "Step 6400/178688 (409600 imgs) >> minibatch loss: 1.7200000286102295; acc 0.25; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 33.27)\n",
      "Step 6450/178688 (412800 imgs) >> minibatch loss: 1.7599999904632568; acc 0.11999999731779099; val_loss: 1.7599999904632568; val_acc 0.20000000298023224 (Total mins: 33.52)\n",
      "Step 6500/178688 (416000 imgs) >> minibatch loss: 1.7100000381469727; acc 0.20000000298023224; val_loss: 1.75; val_acc 0.1899999976158142 (Total mins: 33.77)\n",
      "Step 6550/178688 (419200 imgs) >> minibatch loss: 1.7599999904632568; acc 0.20000000298023224; val_loss: 1.7400000095367432; val_acc 0.20000000298023224 (Total mins: 34.01)\n",
      "Step 6600/178688 (422400 imgs) >> minibatch loss: 1.7300000190734863; acc 0.14000000059604645; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 34.27)\n",
      "Step 6650/178688 (425600 imgs) >> minibatch loss: 1.7300000190734863; acc 0.11999999731779099; val_loss: 1.7599999904632568; val_acc 0.1899999976158142 (Total mins: 34.53)\n",
      "Step 6700/178688 (428800 imgs) >> minibatch loss: 1.7699999809265137; acc 0.2199999988079071; val_loss: 1.75; val_acc 0.20000000298023224 (Total mins: 34.79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1201 15:21:29.108173 140145065584448 ultratb.py:155] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3325, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-01f016bd9dd3>\", line 34, in <module>\n",
      "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.5})\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2039, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    start = dt.now()\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # batch_x, batch_y = iterator.get_next()\n",
    "        batch_x, batch_y = sess.run([train_images, train_labels])\n",
    "        \n",
    "        batch_start = dt.now()\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.5})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy],\n",
    "                             feed_dict={\n",
    "                                 X: batch_x,\n",
    "                                 Y: batch_y,\n",
    "                                 keep_prob: 1.0}\n",
    "                            )\n",
    "            validation_batch_x, validation_batch_y = sess.run(\n",
    "                [validation_images, validation_labels])\n",
    "            \n",
    "            val_loss, val_acc = sess.run([loss_op, accuracy],\n",
    "                                            feed_dict={\n",
    "                                                X: validation_batch_x,\n",
    "                                                Y: validation_batch_y,\n",
    "                                                keep_prob: 1.0\n",
    "                                            })\n",
    "            end_display = (dt.now() - batch_start).total_seconds() / 60\n",
    "            print(f\"Step {step}/{num_steps} ({step*batch_size} imgs) >> minibatch loss: {np.round(loss, 2)}; acc {np.round(acc, 2)}; val_loss: {np.round(val_loss, 2)}; val_acc {np.round(val_acc, 2)} (Total mins: {np.round((dt.now() - start).total_seconds() / 60, 2)})\")\n",
    "\n",
    "    saved_path = saver.save(sess, './tf_api_model', global_step=step)\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy for 256 MNIST test images\n",
    "print(\"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                  Y: mnist.test.labels[:256],\n",
    "                                  keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
