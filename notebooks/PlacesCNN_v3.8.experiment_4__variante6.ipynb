{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from src.models import places_ontop_model\n",
    "from src import custom_losses, custom_metrics, optimizers\n",
    "from src.data import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_classes = 6\n",
    "epochs = 35\n",
    "img_size = 224\n",
    "n_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer block3_conv1 of PlacesCNN has ben set as trainable\n",
      "Layer block3_conv2 of PlacesCNN has ben set as trainable\n",
      "Layer block3_conv3 of PlacesCNN has ben set as trainable\n",
      "Layer block3_pool of PlacesCNN has ben set as trainable\n",
      "Layer block4_conv1 of PlacesCNN has ben set as trainable\n",
      "Layer block4_conv2 of PlacesCNN has ben set as trainable\n",
      "Layer block4_conv3 of PlacesCNN has ben set as trainable\n",
      "Layer block4_pool of PlacesCNN has ben set as trainable\n",
      "Layer block5_conv1 of PlacesCNN has ben set as trainable\n",
      "Layer block5_conv2 of PlacesCNN has ben set as trainable\n",
      "Layer block5_conv3 of PlacesCNN has ben set as trainable\n",
      "Layer block5_pool of PlacesCNN has ben set as trainable\n"
     ]
    }
   ],
   "source": [
    "model = places_ontop_model.PlacesOntop_Model(batch_size, n_classes, epochs, img_size, n_channels, version=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = data.PATH()\n",
    "dataset_path = f'{paths.PROCESSED_DATA_PATH}/'\n",
    "dataset = 'vision_based_dataset'\n",
    "test_dataset_path = f'{dataset_path}/{dataset}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114361 images belonging to 6 classes.\n",
      "Found 6305 images belonging to 6 classes.\n",
      "Found 6328 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator, test_generator = model.get_image_data_generator(test_dataset_path, train=True, validation=True, test=True, class_mode_validation='categorical', class_mode_test='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_class_weights(train_generator.classes, model)\n",
    "model.compile(loss=custom_losses.weighted_categorical_crossentropy(weights), metrics=['categorical_accuracy'],)\n",
    "# model.model.compile(optimizer='adam', loss=custom_losses.weighted_categorical_crossentropy(weights), metrics=['categorical_accuracy'],)\n",
    "# instance_model.compile(optimizer='adam', loss=custom_losses.weighted_categorical_crossentropy(weights), metrics=['categorical_accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### PlacesOntop_Model #####\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "drop_fc1 (Dropout)           (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 14,847,558\n",
      "Trainable params: 14,587,398\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.show_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 23:50:20.835605 140147288192832 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "W0107 23:50:20.836540 140147288192832 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification/src/models/base_model.py:354: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0107 23:50:20.841234 140147288192832 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification/src/models/base_model.py:355: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "W0107 23:50:20.843948 140147288192832 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification/src/models/base_model.py:356: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights [2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n",
      "**** Class weights ****\n",
      "[2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n",
      "*** ** *** *** *** ** ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 23:50:21.153088 140147288192832 deprecation.py:323] From /home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0107 23:50:22.631474 140147288192832 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "894/894 [==============================] - 813s 909ms/step - loss: 1.7920 - categorical_accuracy: 0.1601 - val_loss: 1.5627 - val_categorical_accuracy: 0.1967\n",
      "Epoch 2/35\n",
      "894/894 [==============================] - 823s 921ms/step - loss: 1.7918 - categorical_accuracy: 0.1408 - val_loss: 1.5677 - val_categorical_accuracy: 0.1967\n",
      "Epoch 3/35\n",
      "894/894 [==============================] - 823s 920ms/step - loss: 1.7918 - categorical_accuracy: 0.1750 - val_loss: 1.5658 - val_categorical_accuracy: 0.1981\n",
      "Epoch 4/35\n",
      "894/894 [==============================] - 821s 918ms/step - loss: 1.7918 - categorical_accuracy: 0.1595 - val_loss: 1.5622 - val_categorical_accuracy: 0.1981\n",
      "Epoch 5/35\n",
      "894/894 [==============================] - 817s 914ms/step - loss: 1.7918 - categorical_accuracy: 0.1620 - val_loss: 1.5755 - val_categorical_accuracy: 0.1532\n",
      "Epoch 6/35\n",
      "894/894 [==============================] - 816s 912ms/step - loss: 1.7919 - categorical_accuracy: 0.1644 - val_loss: 1.5671 - val_categorical_accuracy: 0.1967\n",
      "Epoch 7/35\n",
      "894/894 [==============================] - 816s 913ms/step - loss: 1.7919 - categorical_accuracy: 0.1591 - val_loss: 1.5700 - val_categorical_accuracy: 0.1865\n",
      "Epoch 8/35\n",
      "894/894 [==============================] - 818s 915ms/step - loss: 1.7919 - categorical_accuracy: 0.1631 - val_loss: 1.5662 - val_categorical_accuracy: 0.0761\n",
      "Epoch 9/35\n",
      "893/894 [============================>.] - ETA: 0s - loss: 1.7918 - categorical_accuracy: 0.1561Epoch 9/35\n",
      "894/894 [==============================] - 818s 915ms/step - loss: 1.7918 - categorical_accuracy: 0.1562 - val_loss: 1.5634 - val_categorical_accuracy: 0.1967\n",
      "Epoch 10/35\n",
      "894/894 [==============================] - 817s 914ms/step - loss: 1.7919 - categorical_accuracy: 0.1366 - val_loss: 1.5615 - val_categorical_accuracy: 0.1894\n",
      "Epoch 11/35\n",
      "894/894 [==============================] - 817s 914ms/step - loss: 1.7918 - categorical_accuracy: 0.1607 - val_loss: 1.5663 - val_categorical_accuracy: 0.1967\n",
      "Epoch 12/35\n",
      "894/894 [==============================] - 816s 913ms/step - loss: 1.7917 - categorical_accuracy: 0.1214 - val_loss: 1.5663 - val_categorical_accuracy: 0.1967\n",
      "Epoch 13/35\n",
      "894/894 [==============================] - 817s 914ms/step - loss: 1.7918 - categorical_accuracy: 0.1882 - val_loss: 1.5563 - val_categorical_accuracy: 0.1894\n",
      "Epoch 14/35\n",
      "894/894 [==============================] - 815s 912ms/step - loss: 1.7918 - categorical_accuracy: 0.1702 - val_loss: 1.5749 - val_categorical_accuracy: 0.1532\n",
      "Epoch 15/35\n",
      "894/894 [==============================] - 814s 911ms/step - loss: 1.7919 - categorical_accuracy: 0.1576 - val_loss: 1.5619 - val_categorical_accuracy: 0.1865\n",
      "Epoch 16/35\n",
      "894/894 [==============================] - 814s 911ms/step - loss: 1.7918 - categorical_accuracy: 0.1743 - val_loss: 1.5727 - val_categorical_accuracy: 0.1865\n",
      "Epoch 17/35\n",
      "894/894 [==============================] - 818s 915ms/step - loss: 1.7918 - categorical_accuracy: 0.1767 - val_loss: 1.5599 - val_categorical_accuracy: 0.1894\n",
      "Epoch 18/35\n",
      "894/894 [==============================] - 818s 915ms/step - loss: 1.7918 - categorical_accuracy: 0.1698 - val_loss: 1.5525 - val_categorical_accuracy: 0.1894\n",
      "Epoch 19/35\n",
      "894/894 [==============================] - 815s 912ms/step - loss: 1.7918 - categorical_accuracy: 0.1684 - val_loss: 1.5621 - val_categorical_accuracy: 0.0761\n",
      "Epoch 20/35\n",
      "894/894 [==============================] - 814s 910ms/step - loss: 1.7919 - categorical_accuracy: 0.1546 - val_loss: 1.5575 - val_categorical_accuracy: 0.1894\n",
      "Epoch 21/35\n",
      "894/894 [==============================] - 815s 911ms/step - loss: 1.7919 - categorical_accuracy: 0.1527 - val_loss: 1.5627 - val_categorical_accuracy: 0.0761\n",
      "Epoch 22/35\n",
      "894/894 [==============================] - 817s 913ms/step - loss: 1.7918 - categorical_accuracy: 0.1326 - val_loss: 1.5618 - val_categorical_accuracy: 0.1532\n",
      "Epoch 23/35\n",
      "894/894 [==============================] - 814s 910ms/step - loss: 1.7919 - categorical_accuracy: 0.1720 - val_loss: 1.5686 - val_categorical_accuracy: 0.1967\n",
      "Epoch 24/35\n",
      "894/894 [==============================] - 814s 910ms/step - loss: 1.7918 - categorical_accuracy: 0.1710 - val_loss: 1.5597 - val_categorical_accuracy: 0.0761\n",
      "Epoch 25/35\n",
      "894/894 [==============================] - 814s 910ms/step - loss: 1.7918 - categorical_accuracy: 0.1445 - val_loss: 1.5526 - val_categorical_accuracy: 0.1894\n",
      "Epoch 26/35\n",
      "894/894 [==============================] - 813s 910ms/step - loss: 1.7918 - categorical_accuracy: 0.1703 - val_loss: 1.5670 - val_categorical_accuracy: 0.1981\n",
      "Epoch 27/35\n",
      "894/894 [==============================] - 815s 911ms/step - loss: 1.7919 - categorical_accuracy: 0.1733 - val_loss: 1.5681 - val_categorical_accuracy: 0.1532\n",
      "Epoch 28/35\n",
      "894/894 [==============================] - 814s 910ms/step - loss: 1.7919 - categorical_accuracy: 0.1542 - val_loss: 1.5698 - val_categorical_accuracy: 0.1967\n",
      "Epoch 29/35\n",
      "894/894 [==============================] - 815s 911ms/step - loss: 1.7919 - categorical_accuracy: 0.1742 - val_loss: 1.5717 - val_categorical_accuracy: 0.0761\n",
      "Epoch 30/35\n",
      "894/894 [==============================] - 813s 909ms/step - loss: 1.7918 - categorical_accuracy: 0.1555 - val_loss: 1.5621 - val_categorical_accuracy: 0.0761\n",
      "Epoch 31/35\n",
      "894/894 [==============================] - 816s 913ms/step - loss: 1.7918 - categorical_accuracy: 0.1606 - val_loss: 1.5712 - val_categorical_accuracy: 0.1532\n",
      "Epoch 32/35\n",
      "894/894 [==============================] - 813s 909ms/step - loss: 1.7918 - categorical_accuracy: 0.1617 - val_loss: 1.5695 - val_categorical_accuracy: 0.1865\n",
      "Epoch 33/35\n",
      "894/894 [==============================] - 816s 913ms/step - loss: 1.7919 - categorical_accuracy: 0.1641 - val_loss: 1.5679 - val_categorical_accuracy: 0.1981\n",
      "Epoch 34/35\n",
      "894/894 [==============================] - 820s 918ms/step - loss: 1.7918 - categorical_accuracy: 0.1833 - val_loss: 1.5666 - val_categorical_accuracy: 0.1532\n",
      "Epoch 35/35\n",
      "894/894 [==============================] - 818s 915ms/step - loss: 1.7918 - categorical_accuracy: 0.1684 - val_loss: 1.5605 - val_categorical_accuracy: 0.0761\n",
      "Evaluating generator with 6305 images\n",
      "Scores: [1.560486912727356, 0.07613005489110947]\n",
      "\n",
      "Evaluating generator with 6328 images\n",
      "Scores: [1.560486912727356, 0.07743363082408905]\n",
      "\n",
      "weights [2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n"
     ]
    }
   ],
   "source": [
    "model.fit_from_generator(path=f'{dataset_path}/{dataset}', \n",
    "                         train_generator=train_generator, validation_generator=validation_generator,\n",
    "                         test_generator=test_generator,\n",
    "                         evaluate_net=False, use_model_check_point=True, use_early_stop=False, weighted=True,\n",
    "                         show_activations=False, n_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification//models/PlacesOntop_Model/2020-01-05__18_54/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_model(model.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_is_trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n"
     ]
    }
   ],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas:\n",
    "#### - Probar configuraciones 6, 7, 8 y 9.\n",
    "#### - Comparar mejor resultado con notebook placescnn_v2.1\n",
    "#### - Probar configuraciones desfrizando bloques convolutivos de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1215 16:28:29.501663 140671186286400 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "W1215 16:28:29.502433 140671186286400 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification/src/models/base_model.py:352: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W1215 16:28:29.507600 140671186286400 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification/src/models/base_model.py:353: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "W1215 16:28:29.512035 140671186286400 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tf_real_estate_images_classification/src/models/base_model.py:354: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights [2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n",
      "**** Class weights ****\n",
      "[2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n",
      "*** ** *** *** *** ** ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1215 16:28:29.750370 140671186286400 deprecation.py:323] From /home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1215 16:28:30.482717 140671186286400 deprecation_wrapper.py:119] From /home/ifranco/Documents/facultad/tesis/tesis_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "894/894 [==============================] - 1038s 1s/step - loss: 2.4015 - categorical_accuracy: 0.2928 - val_loss: 2.1764 - val_categorical_accuracy: 0.3545\n",
      "Epoch 2/100\n",
      "894/894 [==============================] - 1060s 1s/step - loss: 2.2469 - categorical_accuracy: 0.3689 - val_loss: 2.0527 - val_categorical_accuracy: 0.4125\n",
      "Epoch 3/100\n",
      "894/894 [==============================] - 1051s 1s/step - loss: 2.1950 - categorical_accuracy: 0.3923 - val_loss: 2.1657 - val_categorical_accuracy: 0.4208\n",
      "Epoch 4/100\n",
      "894/894 [==============================] - 1051s 1s/step - loss: 2.1688 - categorical_accuracy: 0.4039 - val_loss: 2.0708 - val_categorical_accuracy: 0.4346\n",
      "Epoch 5/100\n",
      "894/894 [==============================] - 1047s 1s/step - loss: 2.1490 - categorical_accuracy: 0.4133 - val_loss: 2.0666 - val_categorical_accuracy: 0.4411\n",
      "Epoch 6/100\n",
      "894/894 [==============================] - 1049s 1s/step - loss: 2.1337 - categorical_accuracy: 0.4212 - val_loss: 2.1136 - val_categorical_accuracy: 0.4385\n",
      "Epoch 7/100\n",
      "894/894 [==============================] - 1046s 1s/step - loss: 2.1223 - categorical_accuracy: 0.4254 - val_loss: 2.0854 - val_categorical_accuracy: 0.4525\n",
      "Evaluating generator with 6305 images\n",
      "Scores: [2.085378885269165, 0.45249801874160767]\n",
      "\n",
      "Evaluating generator with 6328 images\n",
      "Scores: [2.1083033084869385, 0.4454804062843323]\n",
      "\n",
      "weights [2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n"
     ]
    }
   ],
   "source": [
    "model.fit_from_generator(path=f'{dataset_path}/{dataset}', \n",
    "                         train_generator=train_generator, validation_generator=validation_generator,\n",
    "                         test_generator=test_generator,\n",
    "                         evaluate_net=False, use_model_check_point=True, use_early_stop=True, weighted=True,\n",
    "                         show_activations=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model_is_trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights [2.09728947 0.87794411 1.08704042 0.8524606  0.87239869 0.87343812]\n"
     ]
    }
   ],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_from_generator() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a61bb5334137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_from_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict_from_generator() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "model.predict_from_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
