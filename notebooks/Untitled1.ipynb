{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convolutional Neural Network.\n",
    "Build and train a convolutional neural network with TensorFlow.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "num_classes = 6 # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "filenames = []\n",
    "labels = []\n",
    "\n",
    "train_dir = '/home/ifranco/tf_real_estate_images_classification/data/processed/vision_based_dataset/train/'\n",
    "\n",
    "for d in os.listdir(train_dir):\n",
    "    category_filenames = glob(f'{train_dir}/{d}/*')\n",
    "    filenames.extend(category_filenames)\n",
    "    labels.extend([d for x in range(len(category_filenames))])\n",
    "\n",
    "mapping = {l: idx for idx, l in enumerate(np.unique(labels))}\n",
    "\n",
    "category = [mapping[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = tf.keras.utils.to_categorical(category, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 1\n",
    "filenames = tf.constant(filenames)\n",
    "labels = tf.constant(category)\n",
    "\n",
    "# step 2: create a dataset returning slices of `filenames`\n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 3: parse every image in the dataset using `map`\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.cast(image_decoded, tf.float32)\n",
    "    # resized_images = tf.image.resize_images(images, (img_size, img_size))\n",
    "    resized_image = tf.image.resize_images(image, (img_size, img_size))\n",
    "    # resized_image = tf.reshape(\n",
    "    #     resized_image,\n",
    "    #     (img_size * img_size * 3, ))\n",
    "    return resized_image, label\n",
    "\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.shuffle(5000, reshuffle_each_iteration=True).repeat(100).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4: create iterator and final input tensor\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "images, labels = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "# num_steps = batch_size * num_epochs\n",
    "num_steps = int(100 * 4500 / batch_size) - 1\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "num_input = (img_size * img_size * 3) # MNIST data input (img shape: 28*28)\n",
    "# num_input = (img_size, img_size, 3) # * 128 # MNIST data input (img shape: 28*28)\n",
    "dropout = 0.5 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, img_size, img_size, 3], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes], name='y')\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    # x = tf.reshape(x, shape=[-1, 224, 224, 3]) \n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['block_1_wc1'], biases['block_1_bc1'])\n",
    "    conv1 = tf.compat.v1.layers.batch_normalization(conv1, \n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    conv1 = conv2d(conv1, weights['block_1_wc2'], biases['block_1_bc2'])\n",
    "    conv1 = tf.compat.v1.layers.batch_normalization(conv1,\n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    # Apply Dropout\n",
    "    conv1 = tf.nn.dropout(conv1, 1 if dropout == 1 else 0.75)\n",
    "    \n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['block_2_wc1'], biases['block_2_bc1'])\n",
    "    conv2 = tf.compat.v1.layers.batch_normalization(conv2, \n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    conv2 = conv2d(conv2, weights['block_2_wc2'], biases['block_2_bc2'])\n",
    "    conv2 = tf.compat.v1.layers.batch_normalization(conv2,\n",
    "                                                    training=True if dropout != 1 else False)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    # Apply Dropout\n",
    "    conv2 = tf.nn.dropout(conv2, 1 if dropout == 1 else 0.75)\n",
    "        \n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'block_1_wc1': tf.Variable(tf.random_normal([3, 3, 3, 32])),\n",
    "    'block_1_wc2': tf.Variable(tf.random_normal([3, 3, 32, 32])),\n",
    "    \n",
    "    'block_2_wc1': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
    "    'block_2_wc2': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    \n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*4096, 512])),\n",
    "    # 1024 inputs, 6 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([512, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'block_1_bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'block_1_bc2': tf.Variable(tf.random_normal([32])),\n",
    "    'block_2_bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'block_2_bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([512])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1130 19:46:43.159493 139724735682368 deprecation.py:323] From <ipython-input-9-849358fdedb9>:24: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W1130 19:46:43.269107 139724735682368 deprecation.py:506] From <ipython-input-9-849358fdedb9>:31: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1130 19:46:43.350522 139724735682368 deprecation.py:323] From <ipython-input-10-b9007dae97ad>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=4, keep_checkpoint_every_n_hours=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/7030 (64 imgs) >> minibatch loss: 1665.510009765625; accuracy 0.800000011920929 (Total mins: 0.24. ETA from prev: 0.049974866666666666.)\n",
      "Step 50/7030 (3200 imgs) >> minibatch loss: 0.0; accuracy 1.0 (Total mins: 0.57. ETA from prev: 0.005970016666666666.)\n",
      "Step 100/7030 (6400 imgs) >> minibatch loss: 0.0; accuracy 1.0 (Total mins: 0.9. ETA from prev: 0.005969083333333333.)\n",
      "Step 150/7030 (9600 imgs) >> minibatch loss: 0.0; accuracy 1.0 (Total mins: 1.23. ETA from prev: 0.0059567.)\n",
      "Step 200/7030 (12800 imgs) >> minibatch loss: 7225.14013671875; accuracy 0.949999988079071 (Total mins: 1.56. ETA from prev: 0.005980716666666667.)\n",
      "Step 250/7030 (16000 imgs) >> minibatch loss: 2237.929931640625; accuracy 0.5899999737739563 (Total mins: 1.96. ETA from prev: 0.005983550000000001.)\n",
      "Step 300/7030 (19200 imgs) >> minibatch loss: 763.8599853515625; accuracy 0.7200000286102295 (Total mins: 2.36. ETA from prev: 0.006063799999999999.)\n",
      "Step 350/7030 (22400 imgs) >> minibatch loss: 545.510009765625; accuracy 0.8299999833106995 (Total mins: 2.76. ETA from prev: 0.006005083333333333.)\n",
      "Step 400/7030 (25600 imgs) >> minibatch loss: 466.7900085449219; accuracy 0.8600000143051147 (Total mins: 3.16. ETA from prev: 0.006029366666666666.)\n",
      "Step 450/7030 (28800 imgs) >> minibatch loss: 41.11000061035156; accuracy 0.9399999976158142 (Total mins: 3.55. ETA from prev: 0.00604505.)\n",
      "Step 500/7030 (32000 imgs) >> minibatch loss: 42.540000915527344; accuracy 0.9700000286102295 (Total mins: 3.95. ETA from prev: 0.005988866666666666.)\n",
      "Step 550/7030 (35200 imgs) >> minibatch loss: 434.8599853515625; accuracy 0.8799999952316284 (Total mins: 4.35. ETA from prev: 0.006015049999999999.)\n",
      "Step 600/7030 (38400 imgs) >> minibatch loss: 351.8399963378906; accuracy 0.4699999988079071 (Total mins: 4.74. ETA from prev: 0.006004433333333333.)\n",
      "Step 650/7030 (41600 imgs) >> minibatch loss: 158.16000366210938; accuracy 0.6100000143051147 (Total mins: 5.15. ETA from prev: 0.0060241999999999995.)\n",
      "Step 700/7030 (44800 imgs) >> minibatch loss: 53.45000076293945; accuracy 0.7699999809265137 (Total mins: 5.53. ETA from prev: 0.005988416666666666.)\n",
      "Step 750/7030 (48000 imgs) >> minibatch loss: 48.7599983215332; accuracy 0.9100000262260437 (Total mins: 5.93. ETA from prev: 0.005999383333333333.)\n",
      "Step 800/7030 (51200 imgs) >> minibatch loss: 3.700000047683716; accuracy 0.9200000166893005 (Total mins: 6.32. ETA from prev: 0.006028733333333333.)\n",
      "Step 850/7030 (54400 imgs) >> minibatch loss: 0.0; accuracy 1.0 (Total mins: 6.71. ETA from prev: 0.005983616666666666.)\n",
      "Step 900/7030 (57600 imgs) >> minibatch loss: 213.4499969482422; accuracy 0.550000011920929 (Total mins: 7.15. ETA from prev: 0.006020800000000001.)\n",
      "Step 950/7030 (60800 imgs) >> minibatch loss: 12.270000457763672; accuracy 0.5199999809265137 (Total mins: 7.64. ETA from prev: 0.00600615.)\n",
      "Step 1000/7030 (64000 imgs) >> minibatch loss: 27.670000076293945; accuracy 0.4699999988079071 (Total mins: 8.12. ETA from prev: 0.0059941000000000005.)\n",
      "Step 1050/7030 (67200 imgs) >> minibatch loss: 11.300000190734863; accuracy 0.14000000059604645 (Total mins: 8.5. ETA from prev: 0.006007466666666667.)\n",
      "Step 1100/7030 (70400 imgs) >> minibatch loss: 35.06999969482422; accuracy 0.05999999865889549 (Total mins: 8.83. ETA from prev: 0.00599325.)\n",
      "Step 1150/7030 (73600 imgs) >> minibatch loss: 1.9900000095367432; accuracy 0.09000000357627869 (Total mins: 9.16. ETA from prev: 0.00599155.)\n",
      "Step 1200/7030 (76800 imgs) >> minibatch loss: 2.359999895095825; accuracy 0.029999999329447746 (Total mins: 9.49. ETA from prev: 0.0059747666666666675.)\n",
      "Step 1250/7030 (80000 imgs) >> minibatch loss: 11.770000457763672; accuracy 0.029999999329447746 (Total mins: 9.82. ETA from prev: 0.00599225.)\n",
      "Step 1300/7030 (83200 imgs) >> minibatch loss: 1.690000057220459; accuracy 0.0 (Total mins: 10.15. ETA from prev: 0.0059942166666666664.)\n",
      "Step 1350/7030 (86400 imgs) >> minibatch loss: 1.4800000190734863; accuracy 0.05999999865889549 (Total mins: 10.48. ETA from prev: 0.0060169.)\n",
      "Step 1400/7030 (89600 imgs) >> minibatch loss: 42.970001220703125; accuracy 0.029999999329447746 (Total mins: 10.86. ETA from prev: 0.005999516666666666.)\n",
      "Step 1450/7030 (92800 imgs) >> minibatch loss: 46.20000076293945; accuracy 0.019999999552965164 (Total mins: 11.25. ETA from prev: 0.005991433333333334.)\n",
      "Step 1500/7030 (96000 imgs) >> minibatch loss: 2.880000114440918; accuracy 0.0 (Total mins: 11.64. ETA from prev: 0.005999566666666667.)\n",
      "Step 1550/7030 (99200 imgs) >> minibatch loss: 2.880000114440918; accuracy 0.0 (Total mins: 12.04. ETA from prev: 0.005993.)\n",
      "Step 1600/7030 (102400 imgs) >> minibatch loss: 5.320000171661377; accuracy 0.0 (Total mins: 12.44. ETA from prev: 0.0059900833333333325.)\n",
      "Step 1650/7030 (105600 imgs) >> minibatch loss: 2.7300000190734863; accuracy 0.0 (Total mins: 12.83. ETA from prev: 0.005996183333333334.)\n",
      "Step 1700/7030 (108800 imgs) >> minibatch loss: 2.5799999237060547; accuracy 0.05000000074505806 (Total mins: 13.22. ETA from prev: 0.0059788.)\n",
      "Step 1750/7030 (112000 imgs) >> minibatch loss: 14.40999984741211; accuracy 0.05000000074505806 (Total mins: 13.56. ETA from prev: 0.00598005.)\n",
      "Step 1800/7030 (115200 imgs) >> minibatch loss: 2.2899999618530273; accuracy 0.0 (Total mins: 13.88. ETA from prev: 0.005998816666666667.)\n",
      "Step 1850/7030 (118400 imgs) >> minibatch loss: 2.069999933242798; accuracy 0.0 (Total mins: 14.21. ETA from prev: 0.0059806166666666665.)\n",
      "Step 1900/7030 (121600 imgs) >> minibatch loss: 1.909999966621399; accuracy 0.0 (Total mins: 14.53. ETA from prev: 0.00602395.)\n",
      "Step 1950/7030 (124800 imgs) >> minibatch loss: 1.7699999809265137; accuracy 0.0 (Total mins: 14.86. ETA from prev: 0.005996566666666666.)\n",
      "Step 2000/7030 (128000 imgs) >> minibatch loss: 2.9000000953674316; accuracy 0.0 (Total mins: 15.2. ETA from prev: 0.0060063.)\n",
      "Step 2050/7030 (131200 imgs) >> minibatch loss: 14.130000114440918; accuracy 0.0 (Total mins: 15.6. ETA from prev: 0.00599305.)\n",
      "Step 2100/7030 (134400 imgs) >> minibatch loss: 2.1600000858306885; accuracy 0.0 (Total mins: 15.99. ETA from prev: 0.0059931.)\n",
      "Step 2150/7030 (137600 imgs) >> minibatch loss: 2.200000047683716; accuracy 0.0 (Total mins: 16.38. ETA from prev: 0.005986366666666667.)\n",
      "Step 2200/7030 (140800 imgs) >> minibatch loss: 2.069999933242798; accuracy 0.019999999552965164 (Total mins: 16.77. ETA from prev: 0.00599985.)\n",
      "Step 2250/7030 (144000 imgs) >> minibatch loss: 2.680000066757202; accuracy 0.0 (Total mins: 17.16. ETA from prev: 0.005991283333333333.)\n",
      "Step 2300/7030 (147200 imgs) >> minibatch loss: 1.8700000047683716; accuracy 0.0 (Total mins: 17.55. ETA from prev: 0.005988033333333333.)\n",
      "Step 2350/7030 (150400 imgs) >> minibatch loss: 1.7599999904632568; accuracy 0.0 (Total mins: 17.94. ETA from prev: 0.006059333333333333.)\n",
      "Step 2400/7030 (153600 imgs) >> minibatch loss: 3.1500000953674316; accuracy 0.0 (Total mins: 18.32. ETA from prev: 0.005977133333333333.)\n",
      "Step 2450/7030 (156800 imgs) >> minibatch loss: 1.590000033378601; accuracy 0.8600000143051147 (Total mins: 18.7. ETA from prev: 0.005976400000000001.)\n",
      "Step 2500/7030 (160000 imgs) >> minibatch loss: 1.5199999809265137; accuracy 0.8100000023841858 (Total mins: 19.09. ETA from prev: 0.006048683333333333.)\n",
      "Step 2550/7030 (163200 imgs) >> minibatch loss: 1.3799999952316284; accuracy 0.9700000286102295 (Total mins: 19.47. ETA from prev: 0.00600435.)\n",
      "Step 2600/7030 (166400 imgs) >> minibatch loss: 1.309999942779541; accuracy 0.9399999976158142 (Total mins: 19.86. ETA from prev: 0.00605325.)\n",
      "Step 2650/7030 (169600 imgs) >> minibatch loss: 1.2300000190734863; accuracy 0.9399999976158142 (Total mins: 20.24. ETA from prev: 0.005998716666666667.)\n",
      "Step 2700/7030 (172800 imgs) >> minibatch loss: 13.84000015258789; accuracy 0.6399999856948853 (Total mins: 20.69. ETA from prev: 0.0059941000000000005.)\n",
      "Step 2750/7030 (176000 imgs) >> minibatch loss: 1.7899999618530273; accuracy 0.30000001192092896 (Total mins: 21.17. ETA from prev: 0.005987633333333334.)\n",
      "Step 2800/7030 (179200 imgs) >> minibatch loss: 1.7999999523162842; accuracy 0.17000000178813934 (Total mins: 21.63. ETA from prev: 0.005982983333333333.)\n",
      "Step 2850/7030 (182400 imgs) >> minibatch loss: 1.8200000524520874; accuracy 0.05999999865889549 (Total mins: 21.97. ETA from prev: 0.005991733333333333.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2900/7030 (185600 imgs) >> minibatch loss: 1.7799999713897705; accuracy 0.05999999865889549 (Total mins: 22.3. ETA from prev: 0.005975016666666667.)\n",
      "Step 2950/7030 (188800 imgs) >> minibatch loss: 1.850000023841858; accuracy 0.0 (Total mins: 22.62. ETA from prev: 0.005999183333333334.)\n",
      "Step 3000/7030 (192000 imgs) >> minibatch loss: 3.4200000762939453; accuracy 0.0 (Total mins: 22.95. ETA from prev: 0.0059698166666666665.)\n",
      "Step 3050/7030 (195200 imgs) >> minibatch loss: 2.5799999237060547; accuracy 0.019999999552965164 (Total mins: 23.28. ETA from prev: 0.005987500000000001.)\n",
      "Step 3100/7030 (198400 imgs) >> minibatch loss: 1.4199999570846558; accuracy 0.0 (Total mins: 23.6. ETA from prev: 0.005984183333333334.)\n",
      "Step 3150/7030 (201600 imgs) >> minibatch loss: 1.3300000429153442; accuracy 0.9800000190734863 (Total mins: 23.93. ETA from prev: 0.0060002499999999995.)\n",
      "Step 3200/7030 (204800 imgs) >> minibatch loss: 2.0199999809265137; accuracy 0.4399999976158142 (Total mins: 24.31. ETA from prev: 0.005997416666666667.)\n",
      "Step 3250/7030 (208000 imgs) >> minibatch loss: 2.140000104904175; accuracy 0.30000001192092896 (Total mins: 24.69. ETA from prev: 0.0059880666666666665.)\n",
      "Step 3300/7030 (211200 imgs) >> minibatch loss: 2.2100000381469727; accuracy 0.17000000178813934 (Total mins: 25.08. ETA from prev: 0.006001266666666667.)\n",
      "Step 3350/7030 (214400 imgs) >> minibatch loss: 2.2300000190734863; accuracy 0.05000000074505806 (Total mins: 25.47. ETA from prev: 0.00600455.)\n",
      "Step 3400/7030 (217600 imgs) >> minibatch loss: 2.140000104904175; accuracy 0.019999999552965164 (Total mins: 25.85. ETA from prev: 0.006041983333333333.)\n",
      "Step 3450/7030 (220800 imgs) >> minibatch loss: 5.519999980926514; accuracy 0.019999999552965164 (Total mins: 26.24. ETA from prev: 0.005984616666666666.)\n",
      "Step 3500/7030 (224000 imgs) >> minibatch loss: 1.9199999570846558; accuracy 0.019999999552965164 (Total mins: 26.62. ETA from prev: 0.005991949999999999.)\n",
      "Step 3550/7030 (227200 imgs) >> minibatch loss: 1.8200000524520874; accuracy 0.019999999552965164 (Total mins: 26.94. ETA from prev: 0.005995749999999999.)\n",
      "Step 3600/7030 (230400 imgs) >> minibatch loss: 2.130000114440918; accuracy 0.0 (Total mins: 27.26. ETA from prev: 0.005978949999999999.)\n",
      "Step 3650/7030 (233600 imgs) >> minibatch loss: 1.9700000286102295; accuracy 0.0 (Total mins: 27.59. ETA from prev: 0.005985083333333334.)\n",
      "Step 3700/7030 (236800 imgs) >> minibatch loss: 2.140000104904175; accuracy 0.0 (Total mins: 27.91. ETA from prev: 0.0060067666666666665.)\n",
      "Step 3750/7030 (240000 imgs) >> minibatch loss: 1.7300000190734863; accuracy 0.0 (Total mins: 28.23. ETA from prev: 0.005998433333333333.)\n",
      "Step 3800/7030 (243200 imgs) >> minibatch loss: 1.7899999618530273; accuracy 0.0 (Total mins: 28.59. ETA from prev: 0.006044216666666667.)\n",
      "Step 3850/7030 (246400 imgs) >> minibatch loss: 1.9199999570846558; accuracy 0.0 (Total mins: 28.98. ETA from prev: 0.00601955.)\n",
      "Step 3900/7030 (249600 imgs) >> minibatch loss: 1.899999976158142; accuracy 0.0 (Total mins: 29.37. ETA from prev: 0.00598935.)\n",
      "Step 3950/7030 (252800 imgs) >> minibatch loss: 1.8700000047683716; accuracy 0.0 (Total mins: 29.76. ETA from prev: 0.006009966666666667.)\n",
      "Step 4000/7030 (256000 imgs) >> minibatch loss: 1.75; accuracy 0.0 (Total mins: 30.15. ETA from prev: 0.006060766666666667.)\n",
      "Step 4050/7030 (259200 imgs) >> minibatch loss: 1.659999966621399; accuracy 0.0 (Total mins: 30.54. ETA from prev: 0.005987516666666666.)\n",
      "Step 4100/7030 (262400 imgs) >> minibatch loss: 1.559999942779541; accuracy 0.9800000190734863 (Total mins: 30.92. ETA from prev: 0.00601345.)\n",
      "Step 4150/7030 (265600 imgs) >> minibatch loss: 1.6799999475479126; accuracy 0.5199999809265137 (Total mins: 31.31. ETA from prev: 0.0060243499999999995.)\n",
      "Step 4200/7030 (268800 imgs) >> minibatch loss: 1.7200000286102295; accuracy 0.27000001072883606 (Total mins: 31.69. ETA from prev: 0.005997416666666667.)\n",
      "Step 4250/7030 (272000 imgs) >> minibatch loss: 14.880000114440918; accuracy 0.14000000059604645 (Total mins: 32.07. ETA from prev: 0.006004283333333333.)\n",
      "Step 4300/7030 (275200 imgs) >> minibatch loss: 1.590000033378601; accuracy 0.14000000059604645 (Total mins: 32.45. ETA from prev: 0.005978183333333333.)\n",
      "Step 4350/7030 (278400 imgs) >> minibatch loss: 1.5099999904632568; accuracy 0.07999999821186066 (Total mins: 32.84. ETA from prev: 0.00598385.)\n",
      "Step 4400/7030 (281600 imgs) >> minibatch loss: 1.4199999570846558; accuracy 0.949999988079071 (Total mins: 33.22. ETA from prev: 0.006020333333333333.)\n",
      "Step 4450/7030 (284800 imgs) >> minibatch loss: 1.3300000429153442; accuracy 0.9800000190734863 (Total mins: 33.61. ETA from prev: 0.005991066666666667.)\n",
      "Step 4500/7030 (288000 imgs) >> minibatch loss: 1.940000057220459; accuracy 0.44999998807907104 (Total mins: 34.08. ETA from prev: 0.0060614.)\n",
      "Step 4550/7030 (291200 imgs) >> minibatch loss: 2.069999933242798; accuracy 0.27000001072883606 (Total mins: 34.55. ETA from prev: 0.0059747333333333335.)\n",
      "Step 4600/7030 (294400 imgs) >> minibatch loss: 2.059999942779541; accuracy 0.14000000059604645 (Total mins: 35.0. ETA from prev: 0.0059969.)\n",
      "Step 4650/7030 (297600 imgs) >> minibatch loss: 1.8899999856948853; accuracy 0.11999999731779099 (Total mins: 35.33. ETA from prev: 0.00597755.)\n",
      "Step 4700/7030 (300800 imgs) >> minibatch loss: 1.850000023841858; accuracy 0.05000000074505806 (Total mins: 35.65. ETA from prev: 0.005993983333333333.)\n",
      "Step 4750/7030 (304000 imgs) >> minibatch loss: 1.75; accuracy 0.019999999552965164 (Total mins: 35.98. ETA from prev: 0.0059837499999999995.)\n",
      "Step 4800/7030 (307200 imgs) >> minibatch loss: 1.659999966621399; accuracy 0.0 (Total mins: 36.3. ETA from prev: 0.005988283333333333.)\n",
      "Step 4850/7030 (310400 imgs) >> minibatch loss: 1.5199999809265137; accuracy 0.019999999552965164 (Total mins: 36.63. ETA from prev: 0.005987500000000001.)\n",
      "Step 4900/7030 (313600 imgs) >> minibatch loss: 1.440000057220459; accuracy 0.9800000190734863 (Total mins: 36.95. ETA from prev: 0.006008683333333333.)\n",
      "Step 4950/7030 (316800 imgs) >> minibatch loss: 1.409999966621399; accuracy 0.9200000166893005 (Total mins: 37.29. ETA from prev: 0.005991833333333333.)\n",
      "Step 5000/7030 (320000 imgs) >> minibatch loss: 1.7999999523162842; accuracy 0.4399999976158142 (Total mins: 37.68. ETA from prev: 0.006022733333333334.)\n",
      "Step 5050/7030 (323200 imgs) >> minibatch loss: 1.9900000095367432; accuracy 0.17000000178813934 (Total mins: 38.07. ETA from prev: 0.005981633333333333.)\n",
      "Step 5100/7030 (326400 imgs) >> minibatch loss: 1.9500000476837158; accuracy 0.10999999940395355 (Total mins: 38.45. ETA from prev: 0.0060078833333333335.)\n",
      "Step 5150/7030 (329600 imgs) >> minibatch loss: 1.8799999952316284; accuracy 0.05999999865889549 (Total mins: 38.84. ETA from prev: 0.005994516666666667.)\n",
      "Step 5200/7030 (332800 imgs) >> minibatch loss: 1.7599999904632568; accuracy 0.07999999821186066 (Total mins: 39.23. ETA from prev: 0.0060330666666666664.)\n",
      "Step 5250/7030 (336000 imgs) >> minibatch loss: 1.690000057220459; accuracy 0.019999999552965164 (Total mins: 39.62. ETA from prev: 0.005980333333333333.)\n",
      "Step 5300/7030 (339200 imgs) >> minibatch loss: 1.600000023841858; accuracy 0.0 (Total mins: 39.98. ETA from prev: 0.006002883333333334.)\n",
      "Step 5350/7030 (342400 imgs) >> minibatch loss: 1.5099999904632568; accuracy 0.019999999552965164 (Total mins: 40.31. ETA from prev: 0.0059875833333333335.)\n",
      "Step 5400/7030 (345600 imgs) >> minibatch loss: 2.130000114440918; accuracy 0.0 (Total mins: 40.63. ETA from prev: 0.0060098.)\n",
      "Step 5450/7030 (348800 imgs) >> minibatch loss: 1.9800000190734863; accuracy 0.0 (Total mins: 40.95. ETA from prev: 0.00602065.)\n",
      "Step 5500/7030 (352000 imgs) >> minibatch loss: 1.850000023841858; accuracy 0.0 (Total mins: 41.28. ETA from prev: 0.005990516666666667.)\n",
      "Step 5550/7030 (355200 imgs) >> minibatch loss: 1.7400000095367432; accuracy 0.0 (Total mins: 41.6. ETA from prev: 0.005997183333333334.)\n",
      "Step 5600/7030 (358400 imgs) >> minibatch loss: 1.840000033378601; accuracy 0.0 (Total mins: 41.98. ETA from prev: 0.0060421166666666665.)\n",
      "Step 5650/7030 (361600 imgs) >> minibatch loss: 1.8899999856948853; accuracy 0.0 (Total mins: 42.37. ETA from prev: 0.005974166666666667.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5700/7030 (364800 imgs) >> minibatch loss: 1.8300000429153442; accuracy 0.0 (Total mins: 42.76. ETA from prev: 0.005989316666666666.)\n",
      "Step 5750/7030 (368000 imgs) >> minibatch loss: 1.75; accuracy 0.0 (Total mins: 43.15. ETA from prev: 0.005982749999999999.)\n",
      "Step 5800/7030 (371200 imgs) >> minibatch loss: 1.649999976158142; accuracy 0.05000000074505806 (Total mins: 43.54. ETA from prev: 0.005999866666666667.)\n",
      "Step 5850/7030 (374400 imgs) >> minibatch loss: 1.5499999523162842; accuracy 0.9800000190734863 (Total mins: 43.93. ETA from prev: 0.00598775.)\n",
      "Step 5900/7030 (377600 imgs) >> minibatch loss: 1.4800000190734863; accuracy 0.949999988079071 (Total mins: 44.32. ETA from prev: 0.005985183333333334.)\n",
      "Step 5950/7030 (380800 imgs) >> minibatch loss: 1.6399999856948853; accuracy 0.5600000023841858 (Total mins: 44.7. ETA from prev: 0.005970883333333333.)\n",
      "Step 6000/7030 (384000 imgs) >> minibatch loss: 1.7200000286102295; accuracy 0.3100000023841858 (Total mins: 45.09. ETA from prev: 0.00598335.)\n",
      "Step 6050/7030 (387200 imgs) >> minibatch loss: 1.75; accuracy 0.05999999865889549 (Total mins: 45.47. ETA from prev: 0.006004483333333334.)\n",
      "Step 6100/7030 (390400 imgs) >> minibatch loss: 1.659999966621399; accuracy 0.029999999329447746 (Total mins: 45.85. ETA from prev: 0.005995283333333334.)\n",
      "Step 6150/7030 (393600 imgs) >> minibatch loss: 1.559999942779541; accuracy 0.0 (Total mins: 46.24. ETA from prev: 0.006047316666666667.)\n",
      "Step 6200/7030 (396800 imgs) >> minibatch loss: 1.4600000381469727; accuracy 0.07999999821186066 (Total mins: 46.62. ETA from prev: 0.005975816666666666.)\n",
      "Step 6250/7030 (400000 imgs) >> minibatch loss: 1.6699999570846558; accuracy 0.7699999809265137 (Total mins: 47.03. ETA from prev: 0.005965083333333333.)\n",
      "Step 6300/7030 (403200 imgs) >> minibatch loss: 2.180000066757202; accuracy 0.33000001311302185 (Total mins: 47.5. ETA from prev: 0.005983016666666666.)\n",
      "Step 6350/7030 (406400 imgs) >> minibatch loss: 2.200000047683716; accuracy 0.25 (Total mins: 47.97. ETA from prev: 0.00597315.)\n",
      "Step 6400/7030 (409600 imgs) >> minibatch loss: 2.180000066757202; accuracy 0.05000000074505806 (Total mins: 48.37. ETA from prev: 0.00604315.)\n",
      "Step 6450/7030 (412800 imgs) >> minibatch loss: 2.0299999713897705; accuracy 0.05999999865889549 (Total mins: 48.7. ETA from prev: 0.00597565.)\n",
      "Step 6500/7030 (416000 imgs) >> minibatch loss: 1.8799999952316284; accuracy 0.029999999329447746 (Total mins: 49.02. ETA from prev: 0.005991316666666666.)\n",
      "Step 6550/7030 (419200 imgs) >> minibatch loss: 1.75; accuracy 0.029999999329447746 (Total mins: 49.35. ETA from prev: 0.005987716666666667.)\n",
      "Step 6600/7030 (422400 imgs) >> minibatch loss: 1.6699999570846558; accuracy 0.0 (Total mins: 49.67. ETA from prev: 0.005984983333333333.)\n",
      "Step 6650/7030 (425600 imgs) >> minibatch loss: 1.5199999809265137; accuracy 0.0 (Total mins: 50.0. ETA from prev: 0.006000766666666667.)\n",
      "Step 6700/7030 (428800 imgs) >> minibatch loss: 1.4500000476837158; accuracy 0.9700000286102295 (Total mins: 50.33. ETA from prev: 0.005994466666666667.)\n",
      "Step 6750/7030 (432000 imgs) >> minibatch loss: 1.5199999809265137; accuracy 0.7699999809265137 (Total mins: 50.68. ETA from prev: 0.00600465.)\n",
      "Step 6800/7030 (435200 imgs) >> minibatch loss: 1.8200000524520874; accuracy 0.3100000023841858 (Total mins: 51.06. ETA from prev: 0.005998483333333333.)\n",
      "Step 6850/7030 (438400 imgs) >> minibatch loss: 1.7799999713897705; accuracy 0.27000001072883606 (Total mins: 51.45. ETA from prev: 0.00601655.)\n",
      "Step 6900/7030 (441600 imgs) >> minibatch loss: 1.8200000524520874; accuracy 0.05999999865889549 (Total mins: 51.84. ETA from prev: 0.006025183333333334.)\n",
      "Step 6950/7030 (444800 imgs) >> minibatch loss: 1.7100000381469727; accuracy 0.07999999821186066 (Total mins: 52.23. ETA from prev: 0.005991166666666667.)\n",
      "Step 7000/7030 (448000 imgs) >> minibatch loss: 1.6200000047683716; accuracy 0.029999999329447746 (Total mins: 52.62. ETA from prev: 0.0060300499999999995.)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    start = dt.now()\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # batch_x, batch_y = iterator.get_next()\n",
    "        batch_x, batch_y = sess.run([images, labels])\n",
    "        batch_start = dt.now()\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.5})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy],\n",
    "                             feed_dict={\n",
    "                                 X: batch_x,\n",
    "                                 Y: batch_y,\n",
    "                                 keep_prob: 1.0}\n",
    "                            )\n",
    "            end_display = (dt.now() - batch_start).total_seconds() / 60\n",
    "            print(f\"Step {step}/{num_steps} ({step*batch_size} imgs) >> minibatch loss: {np.round(loss, 2)}; accuracy {np.round(acc, 2)} (Total mins: {np.round((dt.now() - start).total_seconds() / 60, 2)}. ETA from prev: {end_display}.)\")\n",
    "\n",
    "    saved_path = saver.save(sess, './my-model', global_step=step)\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy for 256 MNIST test images\n",
    "print(\"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                  Y: mnist.test.labels[:256],\n",
    "                                  keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
